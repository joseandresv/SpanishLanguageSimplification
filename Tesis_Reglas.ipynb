{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "H2l6pZ2Ovybv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joseandresv/SpanishLanguageSimplification/blob/main/Tesis_Reglas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rules to Code"
      ],
      "metadata": {
        "id": "QvddWtenuhRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Libraries"
      ],
      "metadata": {
        "id": "H2l6pZ2Ovybv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import cess_esp\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download('cess_esp')\n",
        "\n",
        "import re\n",
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "import spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq9XqTMUaNNv",
        "outputId": "d5e1aff3-d818-4452-8cc5-607c52b97457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2023-12-07 00:29:08.967274: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-07 00:29:08.967342: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-07 00:29:08.967389: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-07 00:29:08.977040: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-07 00:29:10.261258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/models/__init__.py\", line 18, in <module>\n",
            "    from keras.src.engine.functional import Functional\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 25, in <module>\n",
            "    from keras.src import backend\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 3312, in <module>\n",
            "    def sin(x):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\", line 1281, in add_dispatch_support\n",
            "    return decorator(target)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\", line 1271, in decorator\n",
            "    op_dispatch_handler = tf_decorator.make_decorator(dispatch_target,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_decorator.py\", line 163, in make_decorator\n",
            "    signature = inspect.signature(target)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 3254, in signature\n",
            "    return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 3002, in from_callable\n",
            "    return _signature_from_callable(obj, sigcls=cls,\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\n",
            "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 146, in _get_module_details\n",
            "    return _get_module_details(pkg_main_name, error)\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 110, in _get_module_details\n",
            "    __import__(pkg_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/errors.py\", line 3, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/compat.py\", line 4, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thinc/config.py\", line 5, in <module>\n",
            "    from .types import Decorator\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thinc/types.py\", line 25, in <module>\n",
            "    from .compat import cupy, has_cupy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thinc/compat.py\", line 54, in <module>\n",
            "    import tensorflow\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 479, in <module>\n",
            "    importlib.import_module(\"keras.optimizers\")\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/__init__.py\", line 3, in <module>\n",
            "    from keras import __internal__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/__internal__/__init__.py\", line 3, in <module>\n",
            "    from keras.__internal__ import backend\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/__internal__/backend/__init__.py\", line 3, in <module>\n",
            "    from keras.src.backend import _initialize_variables as initialize_variables\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/__init__.py\", line 21, in <module>\n",
            "    from keras.src import models\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count Polysemic Words"
      ],
      "metadata": {
        "id": "8Y-JoYgLuoMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to get synonyms of a word\n",
        "def count_words_with_more_than_3_meanings(text):\n",
        "    word_count = 0\n",
        "\n",
        "    # Tokenize the input text\n",
        "    words = nltk.word_tokenize(text, language='spanish')\n",
        "\n",
        "    for word in words:\n",
        "        # Get the number of meanings for the word in Spanish\n",
        "        meaning_count = len(wordnet.synsets(word, lang='spa'))\n",
        "\n",
        "        # Check if the word has more than 3 meanings\n",
        "        if meaning_count > 2:\n",
        "            word_count += 1\n",
        "\n",
        "    return word_count\n",
        "\n",
        "# Example input text in Spanish\n",
        "text = \"Carro, Animal, Zorro, Autómata, Amor, Ficticio.\"\n",
        "\n",
        "# Count words with more than 3 meanings\n",
        "count = count_words_with_more_than_3_meanings(text)\n",
        "\n",
        "# Output the count\n",
        "print(\"Number of words with more than 3 meanings:\", count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44oaLtxvXGpo",
        "outputId": "31ed96a8-db24-4e70-bf3d-b7da688e911a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words with more than 3 meanings: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count Words in a Sentence"
      ],
      "metadata": {
        "id": "3VIwRuR7uyuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_sentences_with_more_than_15_words(text):\n",
        "    # Split the text into sentences using common punctuation marks.\n",
        "    sentences = re.split(r'[.!?]', text)\n",
        "\n",
        "    # Initialize a count for long sentences.\n",
        "    long_sentence_count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Tokenize the sentence into words.\n",
        "        words = sentence.strip().split()\n",
        "\n",
        "        # Count the words in the sentence.\n",
        "        word_count = len(words)\n",
        "\n",
        "        # Check if the sentence has more than 15 words.\n",
        "        if word_count > 15:\n",
        "            long_sentence_count += 1\n",
        "\n",
        "    return long_sentence_count\n",
        "\n",
        "# Example usage:\n",
        "text = \"Este es un ejemplo de una oración que tiene más de quince palabras pero no es cierto. ¿Otro ejemplo? Claro, aquí tienes una oración aún más larga que contiene más de quince palabras. ¡Espero que esto funcione bien!\"\n",
        "count = count_sentences_with_more_than_15_words(text)\n",
        "print(\"Number of sentences with more than 15 words:\", count)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7MFhss5YHE4",
        "outputId": "095c3454-f2c9-431c-fc30-ec2350d0885f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences with more than 15 words: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count Sentences in a Paragraph"
      ],
      "metadata": {
        "id": "k8d-A1gfQwGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_sentences(paragraph):\n",
        "    # Define a regular expression pattern to match sentence endings.\n",
        "    sentence_pattern = r'[.!?]'\n",
        "\n",
        "    # Use the re.split function to split the paragraph into sentences.\n",
        "    sentences = re.split(sentence_pattern, paragraph)\n",
        "\n",
        "    # Filter out empty strings from the result.\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "    # Return the count of sentences.\n",
        "    return len(sentences)\n",
        "\n",
        "# Example usage:\n",
        "paragraph = \"This is a sample paragraph. It contains multiple sentences, like this one! And another? Yes, that's right.\"\n",
        "sentence_count = count_sentences(paragraph)\n",
        "print(\"Number of sentences:\", sentence_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJjEukEMQvWM",
        "outputId": "dd2d8d25-ba32-46e4-cf91-02bbde066cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count Adjectives vs Verbs"
      ],
      "metadata": {
        "id": "-b1WMbDBQ4mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Spacy"
      ],
      "metadata": {
        "id": "NCWMzoxxqfxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Spanish language model\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "def count_verbs(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    verb_count = sum(1 for token in doc if token.pos_ == \"VERB\")\n",
        "    return verb_count\n",
        "\n",
        "def count_adjectives(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    adjective_count = sum(1 for token in doc if token.pos_ == \"ADJ\")\n",
        "    return adjective_count\n",
        "\n",
        "# Example usage:\n",
        "sentence = \"Él corre rápido y salta alto.\"\n",
        "verb_count = count_verbs(sentence)\n",
        "adjective_count = count_adjectives(sentence)\n",
        "print(\"Number of verbs:\", verb_count, \"\\nNumber of adjectives:\", adjective_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtvfrJfHQ4am",
        "outputId": "d793f692-54f3-4fc1-e4a1-ac3a9dcdf2c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of verbs: 1 \n",
            "Number of adjectives: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With NLTK"
      ],
      "metadata": {
        "id": "6RyAPcypqh5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Spanish tagger model\n",
        "cess_esp_data = cess_esp.tagged_sents()\n",
        "print(cess_esp_data)\n",
        "regexp_tagger = nltk.RegexpTagger(cess_esp_data)\n",
        "\n",
        "def count_verbs(sentence):\n",
        "    # Tokenize the sentence\n",
        "    tokens = word_tokenize(sentence)\n",
        "\n",
        "    # Tag the tokens with part-of-speech tags\n",
        "    tagged_tokens = regexp_tagger.tag(tokens)\n",
        "\n",
        "    # Count the number of verbs (VB tags)\n",
        "    verb_count = sum(1 for word, tag in tagged_tokens if tag.startswith('VB'))\n",
        "\n",
        "    return verb_count\n",
        "\n",
        "# Example usage:\n",
        "sentence = \"Él corre rápido y salta alto.\"\n",
        "verb_count = count_verbs(sentence)\n",
        "print(\"Number of verbs:\", verb_count)"
      ],
      "metadata": {
        "id": "UCxGU5QTqjAB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "6925b713-88f4-4ec1-c76c-cf092e90a411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')], [('Una', 'di0fs0'), ('portavoz', 'nccs000'), ('de', 'sps00'), ('EDF', 'np00000'), ('explicó', 'vmis3s0'), ('a', 'sps00'), ('EFE', 'np00000'), ('que', 'cs'), ('el', 'da0ms0'), ('proyecto', 'ncms000'), ('para', 'sps00'), ('la', 'da0fs0'), ('construcción', 'ncfs000'), ('de', 'sps00'), ('Altamira_2', 'np00000'), (',', 'Fc'), ('al', 'spcms'), ('norte', 'ncms000'), ('de', 'sps00'), ('Tampico', 'np00000'), (',', 'Fc'), ('prevé', 'vmm02s0'), ('la', 'da0fs0'), ('utilización', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('natural', 'aq0cs0'), ('como', 'cs'), ('combustible', 'ncms000'), ('principal', 'aq0cs0'), ('en', 'sps00'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('ciclo', 'ncms000'), ('combinado', 'aq0msp'), ('que', 'pr0cn000'), ('debe', 'vmip3s0'), ('empezar', 'vmn0000'), ('a', 'sps00'), ('funcionar', 'vmn0000'), ('en', 'sps00'), ('mayo_del_2002', 'W'), ('.', 'Fp')], ...]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f6e8f6bafc1a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcess_esp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcess_esp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcess_esp_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mregexp_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegexpTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcess_esp_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_verbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, regexps, backoff)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regexps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregexps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regexps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count Abbreviations"
      ],
      "metadata": {
        "id": "PNyPuUK9RQWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_abbreviations(sentence):\n",
        "    # Split the sentence into words\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Define a regular expression pattern to identify potential abbreviations\n",
        "    abbreviation_pattern = r'\\b[A-ZÑ]+\\b'\n",
        "\n",
        "    # Count potential abbreviations\n",
        "    abbreviation_count = 0\n",
        "\n",
        "    for word in words:\n",
        "        if re.match(abbreviation_pattern, word):\n",
        "            abbreviation_count += 1\n",
        "\n",
        "    return abbreviation_count\n",
        "\n",
        "# Example usage:\n",
        "sentence = \"La NASA es una agencia espacial de los EE. UU.\"\n",
        "abbreviation_count = count_abbreviations(sentence)\n",
        "print(\"Number of potential abbreviations:\", abbreviation_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPVeA0D7RiB4",
        "outputId": "5902a166-8347-4b23-f1e6-3beec98bf654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of potential abbreviations: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rule Test"
      ],
      "metadata": {
        "id": "T4qEMASEu7M3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"El Gobierno de México presentó resultados de acciones contundentes en la conservación de la vaquita marina, especie que habita el Alto Golfo de California. Estos logros fueron presentados durante la 77ª sesión del Comité Permanente de la Convención Internacional sobre el Comercio de Especies Amenazadas de Fauna y Flora Silvestres CITES, en la sede de su Secretariado General en Ginebra, Suiza.\"\n"
      ],
      "metadata": {
        "id": "JNFE-yRavIZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este es un texto de prueba para mi proyecto de tésis del ITAM. Para probar las funciones definidas le pasaré un parrafo de texto y a ver que sale. Si una oración es mas larga que cierto número de palabras, o si una palabra es muy compleja (tiene varios significados ex. carro), el problema hace cosas. Estas cosas son las reglas con fundamento en la ISO y otras guías codificadas para evaluación de simplicidad gramatical."
      ],
      "metadata": {
        "id": "V4Re3vI_wwFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rule_1 = count_words_with_more_than_3_meanings(sample_text)\n",
        "rule_2 = count_sentences_with_more_than_15_words(sample_text)\n",
        "rule_3 = count_sentences(sample_text)\n",
        "rule_4 = count_verbs(sample_text)\n",
        "rule_5 = count_adjectives(sample_text)\n",
        "rule_6 = count_abbreviations(sample_text)\n",
        "\n",
        "print(\"polysemic:\", rule_1, \"| sentences > 15 words: \", rule_2, \"| sentences: \", rule_3, \"| verbs: \", rule_4, \"| adjectives: \", rule_5, \"| abbreviations: \", rule_6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZhCbCd3w1so",
        "outputId": "feb64343-3e8d-4d34-a699-5e36c441ccda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "polysemic: 7 | sentences > 15 words:  2 | sentences:  2 | verbs:  3 | adjectives:  2 | abbreviations:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6LljA7bsyO3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}